<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[从零搭建ES搜索服务（六）相关性排序优化]]></title>
    <url>%2F2019%2F01%2F14%2Felasticsearch-6%2F</url>
    <content type="text"><![CDATA[前言上篇介绍了搜索结果高亮的实现方法，本篇主要介绍搜索结果相关性排序优化。 相关概念2.1 排序默认情况下，返回结果是按照「相关性」进行排序的——最相关的文档排在最前。 2.1.1 相关性排序（默认）在 ES 中相关性评分 由一个浮点数表示，并在搜索结果中通过「 _score 」参数返回，默认是按照 _score 降序排列。 2.1.2 按照字段值排序使用「 sort 」参数实现，可指定一个或多个字段。然而使用 sort 排序过于绝对，它会直接忽略文档本身的相关度，因此仅适合在某些特殊场景使用。 注：如果以字符串字段进行排序，除了索引一份用于全文查询的数据，还需要索引一份原始的未经分析器处理（即 not_analyzed ）的数据。这就需要使用「 fields 」参数实现同一个字段多种索引方式，这里的「索引」是动词相当于「存储」的概念。 2.2 相关性算法ES 5.X 版本将相关性算法由之前的「 TF/IDF 」算法改为了更先进的「 BM25 」算法。 2.2.1 TF/IDF 评分算法 ES版本 &lt; 5 的评分算法，即词频/逆向文档频率。 ① 词频（ Term frequency ）搜索词在文档中出现的频率，频率越高，相关度越高。计算公式如下：$$tf(t\ \ in\ \ d) = \sqrt{frequency}$$搜索词「 t 」在文档「 d 」的词频「 tf 」是该词在文档中出现次数的平方根。 ② 逆向文档频率（ Inverse document frequency ）搜索词在索引（单个分片）所有文档里出现的频率，频率越高，相关度越低。用人话描述就是「物以稀为贵」，计算公式如下：$$idf(t) = 1 + log \frac{docCount}{docFreq + 1}$$搜索词「 t 」的逆向文档频率「 idf 」是索引中的文档总数除以所有包含该词的文档数，然后求其对数。 ③ 字段长度归一值（ Field length norm ）字段的长度，字段越短，相关度越高。计算公式如下：$$norm(d) = \frac{1}{\sqrt{numTerms}}$$字段长度归一值「 norm 」是字段中词数平方根的倒数。 注：前面公式中提到的「文档」实际上是指文档里的某个字段 2.2.2 BM25 评分算法 ES版本 &gt;= 5 的评分算法；BM25 的 BM 是缩写自 Best Match， 25 貌似是经过 25 次迭代调整之后得出的算法。它也是基于 TF / IDF 算法进化来的。 对于给定查询语句「Q」，其中包含关键词「$q_{1}$,…$q_{n}$」，那么文档「D」的 BM25 评分计算公式如下：$$score(D,Q) = \sum_{i=1}^NIDF(q_{i})\ ·\ \frac{f(q_{i},D)\ ·\ (k_{1}+1)}{f(q_{i},D)+k_{1}\ ·\ (1-b+b\ ·\ \frac{|D|}{avgdl})}$$这个公式看起来很唬人，尤其是那个求和符号，不过分解开来还是比较好理解的。 总体而言，主要还是分三部分，TF - IDF - Document Length IDF 的计算公式调整为如下所示，其中N 为文档总数， $n(q_{i})$ 为包含搜索词 $q_{i}$ 的文档数。$$IDF(q_{i}) = 1 + log\frac{N-n(q_{i})+0.5}{n(q_{i})+0.5}$$ $f(q_{i},D)$ 为 $q_{i}$ 在文档 D 中的「 TF 」，| D | 是文档的长度，avgdl 是平均文档长度。先不看 IDF 和 Document Length 的部分， 则公式变为 TF * ($k_{1}$ + 1) / (TF + $k_{1}$)，相比传统的 TF/IDF 而言，BM25 抑制了 TF 对整体评分的影响程度，虽然同样都是增函数，但是 BM25 中，TF 越大，带来的影响无限趋近于 ($k_{1}$ + 1)，这里 $k_{1}$ 值通常取 [1.2, 2.0]，而传统的 TF/IDF 则会没有临界点的无限增长。 至于文档长度 | D | 的影响，可以看到在命中搜索词的情况下，文档越短，相关性越高，具体影响程度又可以由公式中的 b 来调整，当设值为 0 的时候，就跟将 norms 设置为 false 一样，忽略文档长度的影响。 最后再对所有搜索词的计算结果求和，就是 ES5 中一般查询的得分了。 实际案例3.1 现实需求要求搜索文章时，搜索词出现在标题时的权重要比出现在内容中高，同时要考虑「引用次数」对最终排序的影响。 3.2 实现方法3.2.1 调整搜索字段权重通过调整字段的 boost 参数实现自定义权重，此处将标题的权重调整为内容的两倍。123456789101112private SearchQuery getKnowledgeSearchQuery(KnowledgeSearchParam param) &#123; ...省略其余部分... BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); boolQuery.must(QueryBuilders.termQuery("isDeleted", IsDeletedEnum.NO.getKey())); boolQuery.should(QueryBuilders.matchQuery(knowledgeTitleFieldName, param.getKeyword()).boost(2.0f)); boolQuery.should(QueryBuilders.matchQuery(knowledgeContentFieldName, param.getKeyword())); return new NativeSearchQueryBuilder() .withPageable(pageable) .withQuery(boolQuery) .withHighlightFields(knowledgeTitleField, knowledgeContentField) .build();&#125; 3.2.2 按引用次数提升权重这里通过 function score 实现重打分操作。根据上面的需求，我们将使用 field value factor 函数指定「 referenceCount 」字段计算分数并与 _score 相加作为最终评分进行排序。1234567891011121314151617private SearchQuery getKnowledgeSearchQuery(KnowledgeSearchParam param) &#123; ...省略其余部分... // 引用次数更多的知识点排在靠前的位置 // 对应的公式为：_score = _score + log (1 + 0.1 * referenceCount) ScoreFunctionBuilder scoreFunctionBuilder = ScoreFunctionBuilders .fieldValueFactorFunction("referenceCount") .modifier(FieldValueFactorFunction.Modifier.LN1P) .factor(0.1f); FunctionScoreQueryBuilder functionScoreQuery = QueryBuilders .functionScoreQuery(boolQuery, scoreFunctionBuilder) .boostMode(CombineFunction.SUM); return new NativeSearchQueryBuilder() .withPageable(pageable) .withQuery(functionScoreQuery) .withHighlightFields(knowledgeTitleField, knowledgeContentField) .build();&#125; 上述的 function score 是 ES 用于处理文档分值的 DSL（领域专用语言），它预定义了一些计算分值的函数： ① weight为每个文档应用一个简单的权重提升值：当 weight 为 2 时，最终结果为 2 * _score ② field_value_factor通过文档中某个字段的值计算出一个分数且使用该值修改 _score，具有以下属性： 属性 描述 field 指定字段名 factor 对字段值进行预处理，乘以指定的数值，默认为 1 modifier 将字段值进行加工，默认为 none boost_mode 控制函数与 _score 合并的结果，默认为 multiply ③ random_score为每个用户都使用一个随机评分对结果排序，可以实现对于用户的个性化推荐。 ④ 衰减函数提供一个更复杂的公式，描述了这样一种情况：对于一个字段，它有一个理想值，而字段实际的值越偏离这个理想值就越不符合期望。具有以下属性： 属性 描述 origin（原点） 该字段的理想值，满分 1.0 offset（偏移量） 与原点相差在偏移量之内的值也可以得到满分 scale（衰减规模） 当值超出原点到偏移量这段范围，它所得的分数就开始衰减，衰减规模决定了分数衰减速度的快慢 decay（衰减值） 该字段可以被接受的值，默认为 0.5 ⑤ script_score支持自定义脚本完全控制评分计算 3.2.3 理解评分标准通过JAVA API 实现相关功能后，输出评分说明可以帮助我们更好的理解评分过程以及后续调整算法参数。 ① 首先定义一个打印搜索结果的方法，设置 explain = true 即可输出 explanation 。1234567891011121314151617181920public void debugSearchQuery(SearchQuery searchQuery, String indexName) &#123; SearchRequestBuilder searchRequestBuilder = elasticsearchTemplate.getClient().prepareSearch(indexName).setTypes(indexName); searchRequestBuilder.setSearchType(SearchType.DFS_QUERY_THEN_FETCH); searchRequestBuilder.setFrom(0).setSize(10); searchRequestBuilder.setExplain(true); searchRequestBuilder.setQuery(searchQuery.getQuery()); SearchResponse searchResponse; try &#123; searchResponse = searchRequestBuilder.execute().get(); long totalCount = searchResponse.getHits().getTotalHits(); log.info("总条数 totalCount:" + totalCount); //遍历结果数据 SearchHit[] hitList = searchResponse.getHits().getHits(); for (SearchHit hit : hitList) &#123; log.info("SearchHit hit explanation:&#123;&#125;\nsource:&#123;&#125;", hit.getExplanation().toString(), hit.getSourceAsString()); &#125; &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125;&#125; ② 之后调用接口，其 explanation 结果展示如下：12345678910111213141516171819202122232419.491358 = sum of 19.309036 = sum of: 19.309036 = sum of: 19.309036 = weight(knowledgeTitle.pinyin:test in 181) [PerFieldSimilarity], result of: 19.309036 = score(doc=181,freq=1.0 = termFreq=1.0), product of: 2.0 = boost 6.2461066 = idf, computed as log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) from: 2.0 = docFreq 1289.0 = docCount 1.5456858 = tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from: 1.0 = termFreq=1.0 1.2 = parameter k1 0.75 = parameter b 29.193172 = avgFieldLength 4.0 = fieldLength 0.0 = match on required clause, product of: 0.0 = # clause 1.0 = isDeleted:[0 TO 0], product of: 1.0 = boost 1.0 = queryNorm 0.18232156 = min of: 0.18232156 = field value function: ln1p(doc['referenceCount'].value * factor=0.1) 3.4028235E38 = maxBoost 其中 idf = 6.2461066，tfNorm = 1.5456858，boost = 2.0，由于此时只有一个搜索字段，因此 score = idf tfNorm boost = 19.309036；与此同时 field value function = 0.18232156；最终得分 sum = 19.309036 + 0.18232156 = 19.491358 。 结语至此一个简单需求的相关性排序优化已经实现完毕，由于业务的关系暂时未涉及其他复杂的场景，所以此篇仅仅作为一个入门介绍。]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零搭建ES搜索服务（五）搜索结果高亮]]></title>
    <url>%2F2019%2F01%2F08%2Felasticsearch-5%2F</url>
    <content type="text"><![CDATA[前言在实际使用中搜索结果中的关键词前端通常会以特殊形式展示，比如标记为红色使人一目了然。我们可以通过 ES 提供的高亮功能实现此效果。 代码实现前文查询是通过一个继承 ElasticsearchRepository 的接口实现的，但是如果要实现高亮，这种方式就满足不了了，这里我们需要通过 ElasticsearchTemplate 来完成。 1.1 注入 ElasticsearchTemplate① ElasticsearchTemplate 类简介123public class ElasticsearchTemplate implements ElasticsearchOperations, ApplicationContextAware &#123; ...省略其余部分...&#125; 从上述源码中可以看到 ElasticsearchTemplate 实现了 ApplicationContextAware 接口，表明这个类是被 Spring 管理的，可以直接注入使用。 ② 业务实现类注入 ElasticsearchTemplate12@Autowiredprivate ElasticsearchTemplate elasticsearchTemplate; 1.2 查询对象指定高亮字段 在构建查询对象时需要指定高亮字段，通过 withHighlightFields 方法设置。 123456789101112131415161718private SearchQuery getKnowledgeSearchQuery(KnowledgeSearchParam param) &#123; Pageable pageable = PageRequest.of(param.getStart() / param.getSize(), param.getSize()); String knowledgeTitleFieldName = "knowledgeTitle"; String knowledgeContentFieldName = "knowledgeContent"; String preTags = "&lt;span style=\"color:#F56C6C\"&gt;"; String postTags = "&lt;/span&gt;"; HighlightBuilder.Field knowledgeTitleField = new HighlightBuilder.Field(knowledgeTitleFieldName).preTags(preTags).postTags(postTags); HighlightBuilder.Field knowledgeContentField = new HighlightBuilder.Field(knowledgeContentFieldName).preTags(preTags).postTags(postTags); BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery(); queryBuilder.must(QueryBuilders.termQuery("isDeleted", IsDeletedEnum.NO.getKey())); queryBuilder.should(QueryBuilders.matchQuery(knowledgeTitleFieldName, param.getKeyword())); queryBuilder.should(QueryBuilders.matchQuery(knowledgeContentFieldName, param.getKeyword())); return new NativeSearchQueryBuilder() .withPageable(pageable) .withQuery(queryBuilder) .withHighlightFields(knowledgeTitleField, knowledgeContentField) .build();&#125; 1.3 自定义 ResultMapper ResultMapper 是用于将 ES 文档转换成 Java 对象的映射类，因为 Spring Data Elasticsearch 默认的的映射类 DefaultResultMapper 不支持高亮，因此，我们需要自定义一个 ResultMapper 。 完整代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576@Slf4j@Componentpublic class HighlightResultHelper implements SearchResultMapper &#123; private static ObjectMapper objectMapper = new ObjectMapper(); static &#123; objectMapper.setVisibility(JsonMethod.FIELD, JsonAutoDetect.Visibility.ANY); objectMapper.configure(SerializationConfig.Feature.INDENT_OUTPUT, true); objectMapper.configure(DeserializationConfig.Feature.FAIL_ON_UNKNOWN_PROPERTIES, false); &#125; private static final Pattern SUB_FIELD_PATTERN = Pattern.compile("\\..*"); private static final String HIGHLIGHT_FIELD_SUFFIX = "Highlight"; @Override public &lt;T&gt; AggregatedPage&lt;T&gt; mapResults(SearchResponse response, Class&lt;T&gt; clazz, Pageable pageable) &#123; long totalHits = response.getHits().getTotalHits(); List&lt;T&gt; list = Lists.newArrayList(); // 获取搜索结果 SearchHits hits = response.getHits(); for (SearchHit searchHit : hits) &#123; if (hits.getHits().length &lt;= 0) &#123; continue; &#125; // 获取高亮字段Map Map&lt;String, HighlightField&gt; highlightFields = searchHit.getHighlightFields(); // 通过jackson将json字符串转化为对象 T item = jsonStrToObject(searchHit.getSourceAsString(), clazz); if (Objects.isNull(item)) &#123; continue; &#125; // 遍历高亮字段Map，将高亮字段key转化为原始字段名（title.pinyin -&gt; title），拼接高亮文本并与原始字段名组装为一个Map Map&lt;String, String&gt; highlightFieldMap = Maps.newHashMap(); for (Map.Entry&lt;String, HighlightField&gt; highlightField : highlightFields.entrySet()) &#123; String key = SUB_FIELD_PATTERN.matcher(highlightField.getKey()).replaceAll(Constants.BLANK) + HIGHLIGHT_FIELD_SUFFIX; HighlightField value = highlightField.getValue(); Text[] fragments = value.getFragments(); StringBuilder sb = new StringBuilder(); for (Text text : fragments) &#123; sb.append(text); &#125; highlightFieldMap.put(key, sb.toString()); &#125; // 通过反射将高亮文本赋值到原始字段对应的高亮字段中 try &#123; Field[] fields = clazz.getDeclaredFields(); for (Field field : fields) &#123; if (!field.getName().contains(HIGHLIGHT_FIELD_SUFFIX)) &#123; continue; &#125; field.setAccessible(true); if (highlightFieldMap.containsKey(field.getName())) &#123; field.set(item, highlightFieldMap.get(field.getName())); &#125; else &#123; field.set(item, searchHit.getSource().get(field.getName().replace(HIGHLIGHT_FIELD_SUFFIX, Constants.BLANK))); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; list.add(item); &#125; return new AggregatedPageImpl&lt;&gt;(list, pageable, totalHits); &#125; private &lt;T&gt; T jsonStrToObject(String json, Class&lt;T&gt; cls) &#123; try &#123; return objectMapper.readValue(json, cls); &#125; catch (IOException e) &#123; log.error("json cant be objectTranslate to object,&#123;&#125;", json); return null; &#125; &#125;&#125; 1.4 获取返回结果① 返回对象增加高亮字段12345678910@Data@Document(indexName = "knowledge", type = "knowledge")public class KnowledgeDO &#123; ...省略其余部分... private String knowledgeTitleHighlight; private String knowledgeContentHighlight;&#125; ② 业务实现类注入 HighlightResultHelper12@Autowiredprivate HighlightResultHelper highlightResultHelper; ③ 获取分页结果由前文的 knowledgeRepository.search 改为 elasticsearchTemplate.queryForPage 实现，查询时指定 highlightResultHelper1Page&lt;KnowledgeDO&gt; page = elasticsearchTemplate.queryForPage(searchQuery, KnowledgeDO.class, highlightResultHelper); 注：测试结果展示 12345678910[ &#123; "id": 850, "knowledgeTitle": "小儿腺样体肥大的孩子宜多吃什么？", "knowledgeTitleHighlight": "小儿腺样体肥大的孩子宜多吃什么？", "knowledgeContent": "1、饮食中要停掉一切寒凉的食物，只吃性平、性温的食物，如猪肉、鸡肉、牛肉、鸽肉、鹌鹑、鳝鱼、泥鳅、青菜、白菜、包菜、黄豆芽、土豆、韭菜、胡萝卜(一周2次)等，夏天再增加四季豆、豇豆、黄瓜、西红柿、藕、芹菜、花菜、各种菌类(菌类也偏凉适合夏天吃)，水果吃新鲜时令的水果，5月份以后，新鲜水果上市了。可以吃草莓、桃子、葡萄、樱桃，秋天可以吃苹果、梨子、桔子等。\n2、每周吃2-3次红烧鳝鱼或喝鳝鱼汤，鳝鱼与其它鱼类不同，补血、补肾、抗过敏的作用明显，但不易上火，补而不燥。每周吃2次海虾，一次10只左右，7岁左右的孩子可以一次半斤，海虾就是鸡尾虾或对虾，补肾阳的作用明显，可以用来治疗慢性扁桃体炎、慢性鼻炎、慢性咽炎，与河虾的功效完全不一样。", "knowledgeContentHighlight": "1、饮食中要停掉一切寒凉的食物，只吃性平、性温的食物，如猪肉、鸡肉、牛肉、鸽肉、鹌鹑、鳝鱼、泥鳅、青菜、白菜、包菜、黄豆芽、土豆、韭菜、胡萝卜(一周2次)等，夏天再增加四季豆、豇豆、黄瓜、&lt;span style=\"color:#F56C6C\"&gt;西红柿&lt;/span&gt;、藕", "referenceCount": 0 &#125;] 结语至此搜索结果高亮已经实现完毕，下一篇将介绍相关度排序优化。]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零搭建ES搜索服务（四）拼音搜索]]></title>
    <url>%2F2019%2F01%2F07%2Felasticsearch-4%2F</url>
    <content type="text"><![CDATA[前言上篇介绍了 ES 的同义词搜索，使我们的搜索更强大了，然而这还远远不够，在实际使用中还可能希望搜索「fanqie」能将包含「番茄」的结果也罗列出来，这就涉及到拼音搜索了，本篇将介绍如何具体实现。 安装 ES 拼音插件1.1 拼音插件简介 GitHub 地址：https://github.com/medcl/elasticsearch-analysis-pinyin 1.2 安装步骤① 进入 ES 的 bin 目录1$ cd /usr/local/elasticsearch/bin/ ② 通过 elasticsearch-plugin 命令安装 pinyin 插件1$ ./elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-pinyin/releases/download/v5.5.3/elasticsearch-analysis-pinyin-5.5.3.zip ③ 安装成功后会在 plugins 目录出现 analysis-pinyin 文件夹 自定义分析器要使用「拼音插件」需要在创建索引时使用「自定义模板」并在自定义模板中「自定义分析器」。 具体配置① 在上篇新建的「 yb_knowledge.json 」模板中修改「 setting 」配置，往其中添加自定义分析器1234567891011121314151617181920212223242526"analysis": &#123; "filter": &#123; ...省略其余部分... "pinyin_filter":&#123; "type": "pinyin", "keep_first_letter": true, "keep_separate_first_letter": false, "keep_full_pinyin": true, "keep_joined_full_pinyin": true, "none_chinese_pinyin_tokenize": false, "keep_joined_full_pinyin": true, "remove_duplicated_term": true, "keep_original": true, "limit_first_letter_length": 50, "lowercase": true &#125; &#125;, "analyzer": &#123; ...省略其余部分... "ik_synonym_pinyin": &#123; "type": "custom", "tokenizer": "ik_smart", "filter": ["synonym_filter","pinyin_filter"] &#125; &#125;&#125; 自定义分析器说明： 首先声明一个新「 token filter 」—— 「 pinyin_filter 」，其中 type 为 pinyin 即拼音插件，其余字段详见 GitHub 项目说明。 其次声明一个新 「analyzer」—— 「ik_synonym_pinyin」，其中 type 为 custom 即自定义类型， tokenizer 为 ik_smart 即使用 ik 分析器的 ik_smart 分词模式， filter 为要使用的词过滤器，可以使用多个，这里使用了上述定义的 pinyin_filter 以及前篇的 synonym_filter 。 ② 与此同时修改「 mappings 」中的 properties 配置，往「 knowledgeTitle 」及「 knowledgeContent 」这两个搜索字段里添加 fields 参数，它支持以不同方式对同一字段做索引，将原本的简单映射转化为多字段映射，此处设置一个名为「 pinyin 」的嵌套字段且使用上述自定义的「 ik_synonym_pinyin 」作为分析器。12345678910111213141516171819202122232425262728"mappings": &#123; "knowledge": &#123; ...省略其余部分... "properties": &#123; ...省略其余部分... "knowledgeTitle": &#123; "type": "text", "analyzer": "ik_synonym_max", "fields":&#123; "pinyin": &#123; "type":"text", "analyzer": "ik_synonym_pinyin" &#125; &#125; &#125;, "knowledgeContent": &#123; "type": "text", "analyzer": "ik_synonym_max", "fields":&#123; "pinyin": &#123; "type":"text", "analyzer": "ik_synonym_pinyin" &#125; &#125; &#125; &#125; &#125;&#125; ③ 最后删除先前创建的 yb_knowledge 索引并重启 Logstash 注：重建索引后可以通过「_analyze」测试分词结果 12345curl -XGET http://localhost:9200/yb_knowledge/_analyze&#123; "analyzer":"ik_synonym_pinyin", "text":"番茄"&#125; 注：在添加了同义词「番茄、西红柿、圣女果」的基础上分词结果如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123&#123; "tokens": [ &#123; "token": "fan", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 0 &#125;, &#123; "token": "番茄", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 0 &#125;, &#123; "token": "fanqie", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 0 &#125;, &#123; "token": "fq", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 0 &#125;, &#123; "token": "qie", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 1 &#125;, &#123; "token": "xi", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 2 &#125;, &#123; "token": "hong", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 3 &#125;, &#123; "token": "shi", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 4 &#125;, &#123; "token": "西红柿", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 4 &#125;, &#123; "token": "xihongshi", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 4 &#125;, &#123; "token": "xhs", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 4 &#125;, &#123; "token": "sheng", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 5 &#125;, &#123; "token": "nv", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 6 &#125;, &#123; "token": "guo", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 7 &#125;, &#123; "token": "圣女果", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 7 &#125;, &#123; "token": "shengnvguo", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 7 &#125;, &#123; "token": "sng", "start_offset": 0, "end_offset": 2, "type": "SYNONYM", "position": 7 &#125; ]&#125; 结语至此拼音搜索已经实现完毕，最近两篇都是有关 ES 插件以及 Logstash 自定义模板的配置，没有涉及具体的 JAVA 代码实现，下一篇将介绍如何通过 JAVA API 实现搜索结果高亮。]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零搭建ES搜索服务（三）同义词搜索]]></title>
    <url>%2F2018%2F12%2F28%2Felasticsearch-3%2F</url>
    <content type="text"><![CDATA[前言上篇介绍了 ES 的基础搜索，能满足我们基本的需求，然而在实际使用中还可能希望搜索「番茄」能将包含「西红柿」的结果也罗列出来，本篇将介绍如何实现同义词之间的搜索。 安装 ES 同义词插件1.1 同义词插件简介 GitHub 地址：https://github.com/ginobefun/elasticsearch-dynamic-synonym 定时从 MySQL 中获取自定义词库，支持「扩展词」及「停用词」 1.2 安装步骤参考 GitHub 中的项目说明 自定义分析器要使用「同义词插件」需要在创建索引时使用「自定义模板」并在自定义模板中「自定义分析器」。 2.1 相关概念① 字符过滤器（character filter）② 分词器（tokenizer）③ 词过滤器（token filter） 自定义分析器官方文档：https://www.elastic.co/guide/cn/elasticsearch/guide/current/custom-analyzers.html 2.2 具体配置① 在上篇新建的「 yb_knowledge.json 」模板中修改「 setting 」配置，往其中添加自定义分析器12345678910111213141516171819202122232425262728"analysis": &#123; "filter": &#123; "synonym_filter": &#123; "type": "dynamic-synonym", "expand": true, "ignore_case": true, "interval": 30, "tokenizer": "ik_max_word", "db_url": "jdbc:mysql://localhost:3306/elasticsearch?user=es_user&amp;password=es_pwd&amp;useUnicode=true&amp;characterEncoding=UTF8" &#125; &#125;, "analyzer": &#123; "ik_synonym_max": &#123; "type": "custom", "tokenizer": "ik_max_word", "filter": [ "synonym_filter" ] &#125;, "ik_synonym_smart": &#123; "type": "custom", "tokenizer": "ik_smart", "filter": [ "synonym_filter" ] &#125; &#125;&#125; 自定义分析器说明： 首先声明一个新「 token filter 」—— 「 synonym_filter 」，其中 type 为 dynamic-synonym 即动态同义词插件， interval 为 定时同步频率（单位为秒）， db_url 为词库的数据库地址。 其次声明一个新 「analyzer」—— 「ik_synonym_max」，其中 type 为 custom 即自定义类型， tokenizer 为 ik_max_word 即使用 ik 分析器的 ik_max_word 分词模式， filter 为要使用的词过滤器，可以使用多个，这里使用了上述定义的 synonym_filter 。 同上继续声明一个以 ik 分析器的 ik_smart 分词模式作为分词器的分析器。 ② 与此同时修改「 mappings 」中的 properties 配置，将「 knowledgeTitle 」及「 knowledgeContent 」这两个字段使用的分析器更换为上述自定义的「 ik_synonym_max 」12345678910111213141516"mappings": &#123; "knowledge": &#123; ...省略其余部分... "properties": &#123; ...省略其余部分... "knowledgeTitle": &#123; "type": "text", "analyzer": "ik_synonym_max" &#125;, "knowledgeContent": &#123; "type": "text", "analyzer": "ik_synonym_max" &#125; &#125; &#125;&#125; ③ 最后删除先前创建的 yb_knowledge 索引并重启 Logstash 注：重建索引后可以通过「_analyze」测试分词结果 ④ 原本在索引中已存在的数据不受同义词动态更新的影响，可以通过以下命令手动更新1curl -XPOST 'http://localhost:9200/yb_knowledge/_update_by_query?conflicts=proceed' 结语至此同义词搜索已经实现完毕，后续将继续介绍其他附加功能，如拼音搜索以及搜索结果高亮等。]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零搭建ES搜索服务（二）基础搜索]]></title>
    <url>%2F2018%2F12%2F27%2Felasticsearch-2%2F</url>
    <content type="text"><![CDATA[前言上篇介绍了 ES 的基本概念及环境搭建，本篇将结合实际需求介绍整个实现过程及核心代码。 安装 ES ik 分析器插件1.1 ik 分析器简介 GitHub 地址：https://github.com/medcl/elasticsearch-analysis-ik 提供两种分词模式：「 ik_max_word 」及「 ik_smart 」 分词模式 描述 ik_max_word 会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合 ik_smart 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌” 1.2 安装步骤① 进入 ES 的 bin 目录1$ cd /usr/local/elasticsearch/bin/ ② 通过 elasticsearch-plugin 命令安装 ik 插件1$ ./elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.3/elasticsearch-analysis-ik-5.5.3.zip ③ 安装成功后会在 plugins 目录出现 analysis-ik 文件夹 数据同步2.1 方案设计通过 Logstash 实现 MySQL 数据库中的数据同步到 ES 中，第一次全量同步，后续每分钟增量同步一次 2.2 实现步骤2.2.1 安装 logstash-input-jdbc 插件① 进入 Logstash 的 bin 目录1$ cd /usr/local/logstash/bin/ ② 使用 logstash-plugin 命令安装 logstash-input-jdbc 插件1$ ./logstash-plugin install logstash-input-jdbc 2.2.2 MySQL 同步数据配置① 首先在 Logstash 安装目录中新建「MySQL 输入数据源」相关目录 /usr/local/logstash/mysql &gt; MySQL 输入数据源目录/usr/local/logstash/mysql/config &gt; 配置文件目录/usr/local/logstash/mysql/metadata &gt; 追踪字段记录文件目录/usr/local/logstash/mysql/statement &gt; SQL 脚本目录 ② 其次上传「MySQL JDBC 驱动」至 /usr/local/logstash/mysql 目录中 mysql-connector-java-5.1.40.jar ③ 然后新建「 SQL 脚本文件」，即 /usr/local/logstash/mysql/statement 目录中新建 yb_knowledge.sql 文件，内容如下：12345678910111213SELECT id, create_time AS createTime, modify_time AS modifyTime, is_deleted AS isDeleted, knowledge_title AS knowledgeTitle, author_name AS authorName, knowledge_content AS knowledgeContent, reference_count AS referenceCountFROM yb_knowledgeWHERE modify_time &gt;= :sql_last_value ④ 之后再新建「配置文件」，即 /usr/local/logstash/mysql/config 目录中新建 yb_knowledge.conf 文件，内容如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849input &#123; jdbc &#123; jdbc_connection_string =&gt; "jdbc:mysql://192.168.1.192:3306/yibao_health" jdbc_user =&gt; "yibao" jdbc_password =&gt; "yibao0415" jdbc_driver_library =&gt; "/usr/local/logstash/mysql/mysql-connector-java-5.1.40.jar" jdbc_driver_class =&gt; "com.mysql.jdbc.Driver" jdbc_paging_enabled =&gt; "true" jdbc_page_size =&gt; "50000" jdbc_default_timezone =&gt; "UTC" lowercase_column_names =&gt; false #使用其它字段追踪，而不是用时间 use_column_value =&gt; true #追踪的字段 tracking_column =&gt; "modifyTime" record_last_run =&gt; true #上一个sql_last_value值的存放文件路径, 必须要在文件中指定字段的初始值 last_run_metadata_path =&gt; "/usr/local/logstash/mysql/metadata/yb_knowledge.txt" # 执行的sql文件路径+名称 statement_filepath =&gt; "/usr/local/logstash/mysql/statement/yb_knowledge.sql" # 设置监听间隔 各字段含义（由左至右）分、时、天、月、年，全部为*默认含义为每分钟都更新 schedule =&gt; "* * * * *" # 索引类型 type =&gt; "knowledge" &#125;&#125;filter &#123; json &#123; source =&gt; "message" remove_field =&gt; ["message"] &#125;&#125;output &#123; if [type] == "knowledge" &#123; elasticsearch &#123; hosts =&gt; ["localhost:9200"] index =&gt; "yb_knowledge" document_id =&gt; "%&#123;id&#125;" &#125; &#125; stdout &#123; # JSON格式输出 codec =&gt; json_lines &#125;&#125; ⑤ 进入 Logstash 的 bin 目录启动，启动时既可指定单个要加载的 conf 文件，也可以指定整个 config 目录1$ ./logstash -f ../mysql/config/yb_knowledge.conf 注：启动 Logstash 时，不管有多少个配置文件最后都会编译成一个文件，也就是说无论有多少个 input 或 output ，最终只有一个 pipeline注：每分钟的 0 秒 Logstash 会自动去同步数据 elasticsearch-head 中可看到最终结果如下： 2.2.3 自定义模板配置此时建立的索引中字符串字段是用的默认分析器 「standard」，会把中文拆分成一个个汉字，这显然不满足我们的需求，所以需要自定义配置以使用 ik 分析器① 首先在 Logstash 安装目录中新建「自定义模板文件」目录 /usr/local/logstash/template &gt; 自定义模板文件目录 ② 其次在该目录中新建 yb_knowledge.json 模板文件，内容如下：12345678910111213141516171819202122232425262728293031323334353637&#123; "template": "yb_knowledge", "settings": &#123; "index.refresh_interval": "5s", "number_of_shards": "1", "number_of_replicas": "1" &#125;, "mappings": &#123; "knowledge": &#123; "_all": &#123; "enabled": false, "norms": false &#125;, "properties": &#123; "@timestamp": &#123; "type": "date", "include_in_all": false &#125;, "@version": &#123; "type": "keyword", "include_in_all": false &#125;, "knowledgeTitle": &#123; "type": "text", "analyzer": "ik_max_word" &#125;, "knowledgeContent": &#123; "type": "text", "analyzer": "ik_max_word" &#125; &#125; &#125; &#125;, "aliases": &#123; "knowledge": &#123;&#125; &#125;&#125; ③ 然后修改 yb_knowledge.conf 文件中的 output 插件，指定要使用的模板文件路径12345678910if [type] == "knowledge" &#123; elasticsearch &#123; hosts =&gt; ["localhost:9200"] index =&gt; "yb_knowledge" document_id =&gt; "%&#123;id&#125;" template_overwrite =&gt; true template =&gt; "/usr/local/logstash/template/yb_knowledge.json" template_name =&gt; "yb_knowledge" &#125; &#125; ④ 之后停止 Logstash 并删除 metadata 目录下 sql_last_value 的存放文件1$ rm -rf /usr/local/logstash/mysql/metadata/yb_knowledge.txt ⑤ 最后删除先前创建的 yb_knowledge 索引并重启 Logstash 注：重建索引后可以通过「_analyze」测试分词结果 2.2.4 自动重载配置文件为了可以自动检测配置文件的变动和自动重新加载配置文件，需要在启动的时候使用以下命令1$ ./logstash -f ../mysql/config/ --config.reload.automatic 默认检测配置文件的间隔时间是 3 秒，可以通过以下命令改变1--config.reload.interval &lt;second&gt; 配置文件自动重载工作原理： 检测到配置文件变化 通过停止所有输入停止当前 pipline （即管道） 用新的配置创建一个新的 pipeline 检查配置文件语法是否正确 检查所有的输入和输出是否可以初始化 检查成功使用新的 pipeline 替换当前的 pipeline 检查失败，使用旧的继续工作 在重载过程中， Logstash 进程没有重启 注：自动重载配置文件不支持 stdin 这种输入类型 代码实现以下代码实现基于 SpringBoot 2.0.4，通过 Spring Data Elasticsearch 提供的 API 操作 ES 3.1 搭建SpringBoot项目 https://www.cnblogs.com/orzlin/p/9717399.html 3.2 引入核心依赖包1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt;&lt;/dependency&gt; 3.3 添加SpringBoot ES相关配置项在application.properties文件中添加es相关配置项123spring.data.elasticsearch.cluster-name = compassspring.data.elasticsearch.cluster-nodes = xxx.xxx.xxx.xxx:9300spring.data.elasticsearch.repositories.enabled = true 3.4 核心代码Spring Data Elasticsearch 提供了类似数据库操作的 repository 接口，可以使我们像操作数据库一样操作 ES① 定义实体123456789101112131415161718192021@Data@Document(indexName = "knowledge", type = "knowledge")public class KnowledgeDO &#123; @Id private Integer id; private Integer isDeleted; private java.time.LocalDateTime createTime; private java.time.LocalDateTime modifyTime; private String knowledgeTitle; private String authorName; private String knowledgeContent; private Integer referenceCount;&#125; ② 定义 repository 接口12public interface KnowledgeRepository extends ElasticsearchRepository&lt;KnowledgeDO, Integer&gt; &#123;&#125; ③ 构建查询对象12345678910111213private SearchQuery getKnowledgeSearchQuery(KnowledgeSearchParam param) &#123; Pageable pageable = PageRequest.of(param.getStart() / param.getSize(), param.getSize()); BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); // 使用 filter 比使用 must query 性能要好 boolQuery.filter(QueryBuilders.termQuery("isDeleted", IsDeletedEnum.NO.getKey())); // 多字段查询 MultiMatchQueryBuilder multiMatchQuery = QueryBuilders.multiMatchQuery(param.getKeyword(), "knowledgeTitle", "knowledgeContent"); boolQuery.must(multiMatchQuery); return new NativeSearchQueryBuilder() .withPageable(pageable) .withQuery(boolQuery) .build();&#125; 注：上述查询类似于 MySQL 中的 select 语句「select * from yb_knowledge where is_deleted = 0 and (knowledge_title like ‘%keyword%’ or knowledge_content like ‘%keyword%’)」 ④ 获取返回结果12SearchQuery searchQuery = getKnowledgeSearchQuery(param);Page&lt;KnowledgeDO&gt; page = knowledgeRepository.search(searchQuery); 注：最终结果默认会按照相关性得分倒序排序，即每个文档跟查询关键词的匹配程度 结语至此一个简易的搜索服务已经实现完毕，后续将继续介绍一些附加功能，如同义词搜索、拼音搜索以及搜索结果高亮等 其它4.1 注意事项① 在 Logstash 的 config 目录执行启动命令时会触发以下错误，所以请移步 bin 目录执行启动命令1ERROR Unable to locate appender "$&#123;sys:ls.log.format&#125;_console" for logger config "root" ② Logstash 中 last_run_metadata_path 文件中保存的 sql_last_value 值是最新一条记录的 tracking_column 值，而不是所有记录中最大的 tracking_column 值③ 当 MySQL 中字段类型为 tinyint(1) 时，同步到 ES 后该字段会转化成布尔类型，改为 tinyint(4) 可避免该问题 4.2 如何使 ES 中的字段名与 Java 实体字段名保持一致？Java 实体字段通常是小驼峰形式命名，而我们数据库表字段都是下划线形式的，所以需要将两者建立映射关系，方法如下：① 修改 statement_filepath 的 SQL 脚本，表字段用 AS 设置成小驼峰式的别名，与 Java 实体字段名保持一致② Logstash 配置文件中的 jdbc 配置还需要加一个配置项 lowercase_column_names =&gt; false ，否则在 ES中字段名默认都是以小写形式存储，不支持驼峰形式 4.3 Logstash 自定义模板详解① 第一次启动 Logstash 时默认会生成一个名叫 「logstash」 的模板到 ES 里，可以通过以下命令查看1curl -XGET 'http://localhost:9200/_template/logstash' 注：默认模板内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&#123; "logstash": &#123; "order": 0, "version": 50001, "template": "logstash-*", "settings": &#123; "index": &#123; "refresh_interval": "5s" &#125; &#125;, "mappings": &#123; "_default_": &#123; "dynamic_templates": [ &#123; "message_field": &#123; "path_match": "message", "mapping": &#123; "norms": false, "type": "text" &#125;, "match_mapping_type": "string" &#125; &#125;, &#123; "string_fields": &#123; "mapping": &#123; "norms": false, "type": "text", "fields": &#123; "keyword": &#123; "ignore_above": 256, "type": "keyword" &#125; &#125; &#125;, "match_mapping_type": "string", "match": "*" &#125; &#125; ], "_all": &#123; "norms": false, "enabled": true &#125;, "properties": &#123; "@timestamp": &#123; "include_in_all": false, "type": "date" &#125;, "geoip": &#123; "dynamic": true, "properties": &#123; "ip": &#123; "type": "ip" &#125;, "latitude": &#123; "type": "half_float" &#125;, "location": &#123; "type": "geo_point" &#125;, "longitude": &#123; "type": "half_float" &#125; &#125; &#125;, "@version": &#123; "include_in_all": false, "type": "keyword" &#125; &#125; &#125; &#125;, "aliases": &#123;&#125; &#125;&#125; ② 使用默认模板，适合刚入门时快速验证使用，但不满足实际需求场景，此时可以在 Logstash 「配置文件」中「 output 」插件中指定自定义模板 覆盖默认模板1234567891011121314151617output &#123; if [type] == "knowledge" &#123; elasticsearch &#123; hosts =&gt; ["localhost:9200"] index =&gt; "yb_knowledge" document_id =&gt; "%&#123;id&#125;" template_overwrite =&gt; true template =&gt; "/usr/local/logstash/template/yb_knowledge.json" template_name =&gt; "yb_knowledge" &#125; &#125; stdout &#123; # JSON格式输出 codec =&gt; json_lines &#125;&#125; 配置项 说明 template_overwrite 是否覆盖默认模板 template 自定义模板文件路径 template_name 自定义模板名 注意事项： 如果不指定「 template_name 」则会永久覆盖默认的「 logstash 」模板，后续即使删除了自定义模板文件，在使用默认模板的情况下创建的索引还是使用先前自定义模板的配置。所以使用自定义模板时建议指定「 template_name 」防止出现一些难以察觉的问题。 如果不小心覆盖了默认模板，需要重置默认模板则执行以下命令后重启 Logstash。 1curl -XDELETE 'http://localhost:9200/_template/logstash' ES 会按照一定的规则来尝试自动 merge 多个都匹配上了的模板规则，最终运用到索引上。所以如果某些自定义模板不再使用记得使用上述命令及时删除，避免新旧版本的模板规则同时作用在索引上引发问题。 例：「 t1 」为旧模板，「 t2 」为新模板，它们的匹配规则一致，唯一的区别是「 t2 」删除了其中一个字段的规则，此时如果「 t1 」模板不删除则新建的索引还是会应用已删除的那条规则。 模板是可以设置 order 参数的，默认的 order 值就是 0。order 值越大，在 merge 模板规则的时候优先级越高。这也是解决新旧版本同一条模板规则冲突的一个解决办法。 ③ 自定义模板中设置索引别名，增加「 aliases 」配置项，如 yb_knowledge =&gt; knowledge12345&quot;template&quot;: &quot;yb_knowledge&quot;,...省略中间部分...&quot;aliases&quot;: &#123; &quot;knowledge&quot;: &#123;&#125;&#125; 4.4 Logstash 多个配置文件里的 input 、filter 、 output 是否相互独立？不独立；Logstash 读取多个配置文件只是简单的将所有配置文件整合到了一起。如果要彼此独立，可以通过 type 或 tags 区分，然后在 output 配置中用 if 语句判断一下 4.5 如何不停机重建索引？① 首先新建新索引「 v2 」② 其次将源索引「 v1 」的数据导入新索引「 v2 」中③ 然后设置索引别名（删除源索引「 v1 」别名，添加新索引「 v2 」别名）④ 之后修改 Logstash 配置文件中 output 的 index 值为新索引「 v2 」 注：前提是 Logstash 启动时指定config.reload.automatic设置项开启配置文件自动重载 ⑤ 再次执行步骤二增量同步源索引 v1中已修改但没同步到新索引「 v2 」中的数据⑥ 最后删除源索引「 v1 」]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零搭建ES搜索服务（一）基本概念及环境搭建]]></title>
    <url>%2F2018%2F12%2F26%2Felasticsearch-1%2F</url>
    <content type="text"><![CDATA[前言本系列文章最终目标是为了快速搭建一个简易可用的搜索服务。方案并不一定是最优，但实现难度较低。 背景近期公司在重构老系统，需求是要求知识库支持全文检索。我们知道普通的数据库like方式效果及性能都不好，所以另寻出路，确定通过 Elasticsearch (下文简称「 ES 」)搜索引擎实现。 技术选型因公司之前购买了阿里云的ES服务且版本为 5.5.3 ，下文选用的技术框架均基于此版本。 ① Elasticsearch 5.5.3 一个基于Lucene的搜索服务器，提供了分布式的全文搜索引擎 ② Logstash 5.5.3 开源的服务器端数据处理管道 ③ Kibana 5.5.3 开源的分析和可视化平台 ④ SpringBoot 2.0.4 系统环境 Linux Centos 7.3 JDK 1.8 基本概念1.1 集群（ cluster ）集群是由一个或者多个拥有相同 cluster.name 配置的节点组成，共同承担数据和负载压力，当节点数量发生变化时集群将会重新平均分布所有数据。 1.2 节点（ node ）一个运行中的 ES 实例称为一个节点 主节点负责管理集群范围内的所有变更，例如增加/删除索引，或者增加/删除节点等，且不需要涉及到文档级别的变更和搜索等操作 任何节点都能成为主节点 当集群只有一个主节点，即使流量增加也不会成为瓶颈 1.3 索引（ index ） 名词；类似于传统关系数据库中的一个数据库 动词；索引一个文档就是存储一个文档到一个索引(名词)中以便它可以被检索和查询到。类似于 SQL 语句中的 INSERT 关键词 倒排索引；类似于传统关系型数据库中的索引概念，可以提升数据检索速度 1.4 类型（ type ）一个索引包含一个或多个 type ，相当于传统关系型数据库中的表 1.5 文档（ document ）相当于传统关系型数据库中的数据行 1.6 分片（ shards ） 是一个底层的「工作单元」，仅保存了全部数据的一部分 是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里 当集群规模扩大或者缩小时， ES 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里 分为「主分片」和「副本分片」 在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改；默认情况下会被分配「 5 」个主分片和「 1 」份副本（每个主分片拥有一个副本分片） 相同主分片的副本分片不会放在同一个节点 ① 主分片 索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量 ② 副本分片（ replicas ） 只是一个主分片的拷贝，作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务 环境搭建2.1 Elasticsearch2.1.1 安装步骤① 下载安装包：1$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.3.tar.gz ② 解压并移动到 local 目录下12$ tar -zxvf elasticsearch-5.5.3.tar.gz$ mv elasticsearch-5.5.3 /usr/local/elasticsearch ③ 修改 config 目录下的 elasticsearch.yml 文件1$ vim elasticsearch.yml 1234// 去掉行开头的 # 并重命名集群名，这里命名为 compasscluster.name: compass// 去掉行开头的 # 并重命名节点名，这里命名为 node-1node.name: node-1 ④ 进入 bin 目录启动 ES 并在后台运行1$ ./elasticsearch -d ⑤ 启动之后测试是否正常运行1$ curl 127.0.0.1:9200 返回结果：12345678910111213&#123; &quot;name&quot; : &quot;node-1&quot;, &quot;cluster_name&quot; : &quot;compass&quot;, &quot;cluster_uuid&quot; : &quot;Zuj5FBMUTjuHQXlAHreGvA&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;5.5.3&quot;, &quot;build_hash&quot; : &quot;9305a5e&quot;, &quot;build_date&quot; : &quot;2017-09-07T15:56:59.599Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 2.1.2 如果提示「-bash: wget: command not found」则需要先安装 wget1$ yum -y install wget 2.1.3 ES 版本&gt; = 5.0.0 时，是不能用超级管理员运行的，此时需要切换到普通账号或者新建 ES 账号 解决办法： ① 新建用户组 elasticsearch1$ groupadd elasticsearch ② 新建用户并指定用户组1$ useradd -g elasticsearch elasticsearch ③ 修改 ES 目录所属者1$ chown -R elasticsearch:elasticsearch elasticsearch ④ 切换用户后再次启动1$ su elasticsearch 2.1.4 只能使用127.0.01或者localhost访问，使用ip地址无法访问？ 解决办法： ① 修改 elasticsearch.yml 中的「network.host」1network.host: 0.0.0.0 ② 重启 ES 出现如果如下报错，请依次按下面的步骤解决1234ERROR: [3] bootstrap checks failed[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536][2]: max number of threads [3818] for user [elasticsearch] is too low, increase to at least [4096][3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] [1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536] 每个进程最大同时打开文件数太小 修改 /etc/security/limits.conf 文件，增加如下配置，用户退出后重新登录生效 12* soft nofile 65536* hard nofile 65536 [2]: max number of threads [3818] for user [es] is too low, increase to at least [4096] 最大线程个数太低 同上修改 /etc/security/limits.conf 文件，增加如下配置，用户退出后重新登录生效 12* soft nproc 4096* hard nproc 4096 [3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 一个进程能拥有的最多的内存区域 修改 /etc/sysctl.conf 文件，增加如下配置，执行命令「 sysctl -p 」生效1vm.max_map_count=262144 ③ 切换到 elasticsearch 用户并重启， curl 测试成功 2.2 Logstash① 下载安装包：1$ wget https://artifacts.elastic.co/downloads/logstash/logstash-5.5.3.tar.gz ② 解压并移动到 local 目录下12$ tar -zxvf logstash-5.5.3.tar.gz$ mv logstash-5.5.3 /usr/local/logstash 2.3 Kibana① 下载安装包：1$ wget https://artifacts.elastic.co/downloads/kibana/kibana-5.5.3-linux-x86_64.tar.gz ② 解压并移动到 local 目录下12$ tar -zxvf kibana-5.5.3-linux-x86_64.tar.gz$ mv kibana-5.5.3-linux-x86_64 /usr/local/kibana ③ 修改 config 目录下的 kibana.yml 文件123456// 去掉当前行开头的 #server.port: 5601// 去掉当前行开头的#并将localhost修改为具体IPserver.host: &quot;192.168.1.191&quot;// 去掉当前行开头的#并将localhost修改为具体IPelasticsearch.url: &quot;http://192.168.1.191:9200&quot; ④ 启动 Kibana ，浏览器访问 http://192.168.1.191:56011$ ./kibana 2.4 elasticsearch-head 插件（浏览器版）① 「 Chrome 浏览器网上应用商店」或者「 Firefox 附加组件」搜索 elasticsearch head ② 安装插件后点击浏览器地址栏右侧「放大镜图标」，顶部输入框中的 localhost 修改为服务器地址即可查看 ES 服务状态 结语至此 ELK 环境搭建完毕，下一篇具体介绍如何实现基础搜索服务。]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F12%2F09%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
